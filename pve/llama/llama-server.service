[Unit]
Description=Llama.cpp Server
After=network.target

[Service]
# Replace 'your_user' with your actual username
# User=root
# Group=root

Type=simple
Restart=on-failure
RestartSec=5s

# This file will hold our model variable
EnvironmentFile=/root/git/nicholaswilde/homelab/pve/llama/.env

# The command to start the server. Note the use of "$LLAMA_MODEL"
# ExecStart=/opt/llama/build/bin/llama-server -m "$LLAMA_MODEL" --host 0.0.0.0 --port 8080 -ngl 0 -c 4096
ExecStart=/opt/llama/build/bin/llama-server --host 0.0.0.0 --port 8080 -ngl 0 -c 8192 --timeout 0
# ExecStart=/opt/llama/build/bin/llama-server -hf TheBloke/deepseek-coder-6.7B-instruct-GGUF:Q5_K_M  --host 0.0.0.0 --port 8080 -ngl 0 -c 4096

[Install]
WantedBy=multi-user.target
